import streamlit as st
import google.generativeai as genai
from openai import OpenAI

# --- 1. é¡µé¢è®¾ç½® ---
st.set_page_config(page_title="å…¨èƒ½ AI èšåˆåŠ©æ‰‹", page_icon="ğŸ¤–", layout="wide")
st.title("ğŸ¤– å…¨èƒ½ AI èšåˆåŠ©æ‰‹")

# --- 2. ä¾§è¾¹æ ï¼šæ§åˆ¶ä¸­å¿ƒ ---
with st.sidebar:
    st.header("ğŸ® æ¨¡å‹æ§åˆ¶å°")
    
    # é€‰æ‹©å‚å•†
    provider = st.selectbox(
        "1. é€‰æ‹©å‚å•†",
        ["Google Gemini", "DeepSeek (æ·±åº¦æ±‚ç´¢)", "é˜¿é‡Œé€šä¹‰åƒé—®", "å­—èŠ‚è±†åŒ… (ç«å±±å¼•æ“)"]
    )

    # -------------------------------------------------------
    # é€»è¾‘ A: Google Gemini (ç‹¬ç«‹é€»è¾‘)
    # -------------------------------------------------------
    if provider == "Google Gemini":
        # å°è¯•ä» Secrets è·å– Keyï¼Œå¦‚æœæ²¡æœ‰åˆ™æ˜¾ç¤ºè¾“å…¥æ¡†
        default_key = st.secrets.get("GOOGLE_API_KEY", "")
        api_key = st.text_input("è¾“å…¥ Gemini API Key", value=default_key, type="password")
        
        # ç®€å•çš„æ¨¡å‹åˆ—è¡¨ (å› ä¸ºè‡ªåŠ¨æ£€ç´¢éœ€è¦å…ˆéªŒè¯Keyï¼Œä¸ºäº†ä¸æŠ¥é”™ï¼Œæˆ‘ä»¬é¢„è®¾å¸¸ç”¨åˆ—è¡¨)
        model_list = ["models/gemini-1.5-flash", "models/gemini-1.5-pro", "models/gemini-3-pro-preview"]
        selected_model = st.selectbox("2. é€‰æ‹©æ¨¡å‹", model_list)
        
        # æ·±åº¦æ€è€ƒå¼€å…³
        is_deep_think = st.toggle("ğŸ§  å¼€å¯æ·±åº¦æ€è€ƒæ¨¡å¼", value=False)

    # -------------------------------------------------------
    # é€»è¾‘ B: å›½äº§æ¨¡å‹ (ç»Ÿç”¨ OpenAI æ ¼å¼è¿æ¥)
    # -------------------------------------------------------
    else:
        # æ ¹æ®å‚å•†é¢„è®¾ Base URL å’Œ æ¨¡å‹åˆ—è¡¨
        if provider == "DeepSeek (æ·±åº¦æ±‚ç´¢)":
            base_url = "https://api.deepseek.com"
            default_key = st.secrets.get("DEEPSEEK_API_KEY", "")
            # DeepSeek åªæœ‰è¿™ä¸¤ä¸ªä¸»è¦æ¨¡å‹
            model_options = ["deepseek-chat", "deepseek-coder"]
            help_text = "Key è·å–åœ°å€: platform.deepseek.com"
            
        elif provider == "é˜¿é‡Œé€šä¹‰åƒé—®":
            base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
            default_key = st.secrets.get("DASHSCOPE_API_KEY", "")
            # é˜¿é‡Œå¸¸ç”¨æ¨¡å‹
            model_options = ["qwen-plus", "qwen-max", "qwen-turbo"]
            help_text = "Key è·å–åœ°å€: bailian.console.aliyun.com"
            
        elif provider == "å­—èŠ‚è±†åŒ… (ç«å±±å¼•æ“)":
            base_url = "https://ark.cn-beijing.volces.com/api/v3"
            default_key = st.secrets.get("VOLC_API_KEY", "")
            # è±†åŒ…æ¯”è¾ƒç‰¹æ®Šï¼Œè¿™é‡Œä¸åˆ—åå­—ï¼Œç”¨æˆ·å¿…é¡»å¡« Endpoint ID
            model_options = ["æ‰‹åŠ¨è¾“å…¥ Endpoint ID"] 
            help_text = "âš ï¸ è±†åŒ…å¿…é¡»å¡«å†™ 'ep-xxx' å¼€å¤´çš„æ¥å…¥ç‚¹ IDï¼Œè€Œéæ¨¡å‹åã€‚"

        # æ˜¾ç¤º Key è¾“å…¥æ¡†
        api_key = st.text_input(f"è¾“å…¥ {provider} API Key", value=default_key, type="password", help=help_text)
        
        # æ˜¾ç¤ºæ¨¡å‹é€‰æ‹©
        if provider == "å­—èŠ‚è±†åŒ… (ç«å±±å¼•æ“)":
            selected_model = st.text_input("è¾“å…¥è±†åŒ… Endpoint ID (ep-xxxx...)", value=st.secrets.get("DOUBAO_ENDPOINT_ID", ""))
        else:
            selected_model = st.selectbox("2. é€‰æ‹©æ¨¡å‹", model_options)

    # æ¸…ç©ºæŒ‰é’®
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯"):
        st.session_state.messages = []
        st.rerun()

# --- 3. èŠå¤©ç•Œé¢ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- 4. å¤„ç†è¾“å…¥ ---
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    if not api_key:
        st.error("ğŸš« è¯·å…ˆåœ¨å·¦ä¾§å¡«å†™ API Keyï¼")
        st.stop()

    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # --- 5. æ ¸å¿ƒç”Ÿæˆé€»è¾‘ (åŒè½¨åˆ¶) ---
    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ""
        
        try:
            # === åˆ†æ”¯ä¸€ï¼šGemini å¤„ç† ===
            if provider == "Google Gemini":
                genai.configure(api_key=api_key)
                
                sys_prompt = "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚"
                if is_deep_think:
                    sys_prompt = "ä½ æ˜¯ä¸€ä¸ªæ·±åº¦æ€è€ƒä¸“å®¶ã€‚å›ç­”å‰è¯·å…ˆåœ¨ <thinking> æ ‡ç­¾ä¸­è¿›è¡Œè¯¦ç»†æ¨æ¼”ã€‚"
                
                model = genai.GenerativeModel(selected_model, system_instruction=sys_prompt)
                response = model.generate_content(prompt, stream=True)
                
                for chunk in response:
                    if chunk.text:
                        full_response += chunk.text
                        placeholder.markdown(full_response + "â–Œ")

            # === åˆ†æ”¯äºŒï¼šå›½äº§æ¨¡å‹ (OpenAI å…¼å®¹æ¨¡å¼) ===
            else:
                client = OpenAI(api_key=api_key, base_url=base_url)
                
                # æ„é€ æ¶ˆæ¯å†å²
                messages = [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages]
                
                stream = client.chat.completions.create(
                    model=selected_model,
                    messages=messages,
                    stream=True
                )
                
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        placeholder.markdown(full_response + "â–Œ")

            placeholder.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})

        except Exception as e:
            st.error(f"è¯·æ±‚å¤±è´¥: {e}")
            st.warning("å¦‚æœè¿æ¥å›½äº§æ¨¡å‹è¶…æ—¶ï¼Œå¯èƒ½æ˜¯å› ä¸ºäº‘ç«¯æœåŠ¡å™¨ç½‘ç»œæ³¢åŠ¨ã€‚")
